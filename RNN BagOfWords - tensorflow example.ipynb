{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Example of Estimator for DNN-based text classification with DBpedia data.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "import csv\n",
    "\n",
    "import os\n",
    "\n",
    "import tarfile\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.contrib.learn.python.learn.datasets import text_datasets\n",
    "\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "\n",
    "from tensorflow.python.platform import gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = None\n",
    "MAX_DOCUMENT_LENGTH = 10\n",
    "EMBEDDING_SIZE = 50\n",
    "n_words = 0\n",
    "MAX_LABEL = 15\n",
    "WORDS_FEATURE = 'words'  # Name of the input words feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimator_spec_for_softmax_classification(logits, labels, mode):\n",
    "\n",
    "  \"\"\"Returns EstimatorSpec instance for softmax classification.\"\"\"\n",
    "  predicted_classes = tf.argmax(logits, 1)\n",
    "    \n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions={\n",
    "            'class': predicted_classes,\n",
    "            'prob': tf.nn.softmax(logits)\n",
    "        })\n",
    "\n",
    "  onehot_labels = tf.one_hot(labels, MAX_LABEL, 1, 0)\n",
    "\n",
    "  loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  eval_metric_ops = {\n",
    "      'accuracy': tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predicted_classes)\n",
    "  }\n",
    "\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_words_model(features, target):\n",
    "\n",
    "  \"\"\"A bag-of-words model. Note it disregards the word order in the text.\"\"\"\n",
    "  bow_column = tf.feature_column.categorical_column_with_identity(WORDS_FEATURE, num_buckets=n_words)\n",
    "\n",
    "  bow_embedding_column = tf.feature_column.embedding_column(bow_column, dimension=EMBEDDING_SIZE)\n",
    "\n",
    "  bow = tf.feature_column.input_layer(features, feature_columns=[bow_embedding_column])\n",
    "\n",
    "  logits = tf.layers.dense(bow, MAX_LABEL, activation=None)\n",
    "\n",
    "  return estimator_spec_for_softmax_classification(\n",
    "      logits=logits, labels=labels, mode=mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_model(features, labels, mode):\n",
    "\n",
    "  \"\"\"RNN model to predict from sequence of words to a class.\"\"\"\n",
    "  # Convert indexes of words into embeddings.\n",
    "  # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "  # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "  # EMBEDDING_SIZE].\n",
    "\n",
    "  word_vectors = tf.contrib.layers.embed_sequence(\n",
    "      features[WORDS_FEATURE], vocab_size=n_words, embed_dim=EMBEDDING_SIZE)\n",
    "\n",
    "  # Split into list of embedding per word, while removing doc length dim.\n",
    "  # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "\n",
    "  word_list = tf.unstack(word_vectors, axis=1)\n",
    "\n",
    "  # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "  cell = tf.contrib.rnn.GRUCell(EMBEDDING_SIZE)\n",
    "\n",
    "  # Create an unrolled Recurrent Neural Networks to length of\n",
    "  # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "  _, encoding = tf.contrib.rnn.static_rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "  # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "  # neural network of last step) and pass it as features for softmax\n",
    "  # classification over output classes.\n",
    "\n",
    "  logits = tf.layers.dense(encoding, MAX_LABEL, activation=None)\n",
    "\n",
    "  return estimator_spec_for_softmax_classification(\n",
    "      logits=logits, labels=labels, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def shrink_csv(filename, ratio):\n",
    "\n",
    "#  \"\"\"Create a smaller dataset of only 1/ratio of original data.\"\"\"\n",
    "\n",
    " # filename_small = filename.replace('.', '_medium.')\n",
    "\n",
    "  #with gfile.Open(filename_small, 'w') as csv_file_small:\n",
    "\n",
    "   # writer = csv.writer(csv_file_small)\n",
    "\n",
    "    #with gfile.Open(filename) as csv_file:\n",
    "\n",
    "     # reader = csv.reader(csv_file)\n",
    "\n",
    "      #i = 0\n",
    "\n",
    "     # for row in reader:\n",
    "\n",
    "      #  if i % ratio == 0:\n",
    "\n",
    "       #   writer.writerow(row)\n",
    "\n",
    "       # i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = os.path.join(os.getenv('TF_EXP_BASE_DIR', ''), 'dbpedia_data')\n",
    "\n",
    "\n",
    "#train_path = os.path.join(data_dir, 'dbpedia_csv/train.csv')\n",
    "\n",
    "#test_path = os.path.join(data_dir, 'dbpedia_csv/test.csv')\n",
    "\n",
    "#train_path = os.path.join(data_dir, 'dbpedia_csv', 'train.csv')\n",
    "#test_path = os.path.join(data_dir, 'dbpedia_csv', 'test.csv')\n",
    "\n",
    "# Reduce the size of original data by a factor of 1000.\n",
    "\n",
    "#shrink_csv(train_path, 8)\n",
    "\n",
    "#shrink_csv(test_path, 8)\n",
    "\n",
    "\n",
    "\n",
    "#train_path = train_path.replace('train.csv', 'train_medium.csv')\n",
    "\n",
    "#test_path = test_path.replace('test.csv', 'test_medium.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dbpedia(size='small', test_with_fake_data=False):\n",
    "  \"\"\"Get DBpedia datasets from CSV files.\"\"\"\n",
    "  if not test_with_fake_data:\n",
    "    data_dir = os.path.join(os.getenv('TF_EXP_BASE_DIR', ''), 'dbpedia_data')\n",
    "    text_datasets.maybe_download_dbpedia(data_dir)\n",
    "\n",
    "    train_path = os.path.join(data_dir, 'dbpedia_csv', 'train.csv')\n",
    "    test_path = os.path.join(data_dir, 'dbpedia_csv', 'test.csv')\n",
    "\n",
    "    if size == 'small':\n",
    "      # Reduce the size of original data by a factor of 1000.\n",
    "      base.shrink_csv(train_path, 7)\n",
    "      base.shrink_csv(test_path, 7)\n",
    "      train_path = train_path.replace('train.csv', 'train_small.csv')\n",
    "      test_path = test_path.replace('test.csv', 'test_small.csv')\n",
    "\n",
    "  else:\n",
    "    module_path = os.path.dirname(__file__)\n",
    "    train_path = os.path.join(module_path, 'data', 'text_train.csv')\n",
    "    test_path = os.path.join(module_path, 'data', 'text_test.csv')\n",
    "\n",
    "  train = base.load_csv_without_header(\n",
    "      train_path, target_dtype=np.int32, features_dtype=np.str, target_column=0)\n",
    "  test = base.load_csv_without_header(\n",
    "      test_path, target_dtype=np.int32, features_dtype=np.str, target_column=0)\n",
    "\n",
    "  return base.Datasets(train=train, validation=None, test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "\n",
    "  global n_words\n",
    "\n",
    "  # Prepare training and testing data\n",
    "#  dbpedia = tf.contrib.learn.datasets.load_dataset(\n",
    "#      'dbpedia',test_with_fake_data=FLAGS.test_with_fake_data)\n",
    "\n",
    "  dbpedia = load_dbpedia(size='small', test_with_fake_data=FLAGS.test_with_fake_data)\n",
    "\n",
    "  x_train = pandas.Series(dbpedia.train.data[:,1])\n",
    "  y_train = pandas.Series(dbpedia.train.target)\n",
    "  x_test = pandas.Series(dbpedia.test.data[:,1])\n",
    "  y_test = pandas.Series(dbpedia.test.target)\n",
    "\n",
    "  # Process vocabulary\n",
    "  vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
    "      MAX_DOCUMENT_LENGTH)\n",
    "\n",
    "  x_train = np.array(list(vocab_processor.fit_transform(x_train)))\n",
    "  x_test = np.array(list(vocab_processor.transform(x_test)))\n",
    "\n",
    "  n_words = len(vocab_processor.vocabulary_)\n",
    "  print('Total words: %d' % n_words)\n",
    "\n",
    "  # Build model\n",
    "  # Switch between rnn_model and bag_of_words_model to test different models.\n",
    "  model_fn = rnn_model\n",
    "\n",
    "  if FLAGS.bow_model:\n",
    "    # Subtract 1 because VocabularyProcessor outputs a word-id matrix where word\n",
    "    # ids start from 1 and 0 means 'no word'. But\n",
    "    # categorical_column_with_identity assumes 0-based count and uses -1 for\n",
    "    # missing word.\n",
    "    x_train -= 1\n",
    "    x_test -= 1\n",
    "    model_fn = bag_of_words_model\n",
    "\n",
    "  classifier = tf.estimator.Estimator(model_fn=model_fn)\n",
    "\n",
    "  # Train.\n",
    "  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={WORDS_FEATURE: x_train},\n",
    "      y=y_train,\n",
    "      batch_size=len(x_train),\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "\n",
    "  classifier.train(input_fn=train_input_fn, steps=100)\n",
    "\n",
    "  # Predict.\n",
    "  test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={WORDS_FEATURE: x_test},\n",
    "      y=y_test,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "\n",
    "  predictions = classifier.predict(input_fn=test_input_fn)\n",
    "  y_predicted = np.array(list(p['class'] for p in predictions))\n",
    "  y_predicted = y_predicted.reshape(np.array(y_test).shape)\n",
    "\n",
    "  # Score with sklearn.\n",
    "  score = metrics.accuracy_score(y_test, y_predicted)\n",
    "  print('Accuracy (sklearn): {0:f}'.format(score))\n",
    "\n",
    "  # Score with tensorflow.\n",
    "  scores = classifier.evaluate(input_fn=test_input_fn)\n",
    "  print('Accuracy (tensorflow): {0:f}'.format(scores['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 228987\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Vilma\\AppData\\Local\\Temp\\tmp6k5mi3u1\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\Vilma\\AppData\\Local\\Temp\\tmp6k5mi3u1\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.70789, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into C:\\Users\\Vilma\\AppData\\Local\\Temp\\tmp6k5mi3u1\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00101484.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Vilma\\AppData\\Local\\Temp\\tmp6k5mi3u1\\model.ckpt-100\n",
      "Accuracy (sklearn): 0.800400\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-08-23:21:34\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Vilma\\AppData\\Local\\Temp\\tmp6k5mi3u1\\model.ckpt-100\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-08-23:21:35\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.8004, global_step = 100, loss = 1.13198\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "Accuracy (tensorflow): 0.800400\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vilma\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2855: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--test_with_fake_data',\n",
    "      default=False,\n",
    "      help='Test the example code with fake data.',\n",
    "      action='store_true')\n",
    "\n",
    "  parser.add_argument(\n",
    "      '--bow_model',\n",
    "      default=False,\n",
    "      help='Run with BOW model instead of RNN.',\n",
    "      action='store_true')\n",
    "\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
